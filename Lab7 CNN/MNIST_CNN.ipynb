{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5OQcIz2_I_CE"
   },
   "outputs": [],
   "source": [
    "#SOURCE: https://victorzhou.com/blog/keras-cnn-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VctQ9QmZI_CJ",
    "outputId": "3130e71f-1d2a-4d45-d6a2-a38d37e5d019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mnist\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: numpy in /home/sajjad/.local/lib/python3.8/site-packages (from mnist) (1.22.3)\n",
      "Installing collected packages: mnist\n",
      "Successfully installed mnist-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgkgkg8rI_CO",
    "outputId": "27ce890e-0a27-48c9-f6bc-1a8614e8dfc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "from tensorflow import keras\n",
    "\n",
    "# The first time you run this might be a bit slow, since the\n",
    "# mnist package has to download and cache the data.\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "print(train_images.shape) # (60000, 28, 28)\n",
    "print(train_labels.shape) # (60000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "fUZmaU7FJm8Y",
    "outputId": "f00a1cbc-95d4-40e5-bb07-d49755ed1edb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5aa87c0670>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aUkcxoVI_CR"
   },
   "source": [
    "Preparing the Data\n",
    "\n",
    "Before we begin, we’ll normalize the image pixel values from [0, 255] to [-0.5, 0.5] to make our network easier to train (using smaller, centered values usually leads to better results). We’ll also reshape each image from (28, 28) to (28, 28, 1) because Keras requires the third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeuSwDIYI_CR",
    "outputId": "7b96c6e4-70b4-4481-f195-c1cf54f45309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "print(train_images.shape) # (60000, 28, 28, 1)\n",
    "print(test_images.shape)  # (10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJuEZMxqI_CW",
    "outputId": "b048cd76-bc59-44ec-c3c9-647d7da572b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy4UUwlAI_CZ",
    "outputId": "28c4cbee-599e-4d0a-fd24-ca8eab64d786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot encoding\n",
    "\n",
    "#Do it yourself!\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FB9W4fBoI_Cb"
   },
   "outputs": [],
   "source": [
    "#Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XU8w2Y7qI_Cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 20:16:30.394920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1boVnpWdI_Cf"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWN0v_uJI_Ch",
    "outputId": "15fe9227-2df3-4a15-ccb9-ae4692b325c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3340 - accuracy: 0.9032 - val_loss: 0.1969 - val_accuracy: 0.9409\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1641 - accuracy: 0.9524 - val_loss: 0.1290 - val_accuracy: 0.9618\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1231 - accuracy: 0.9642 - val_loss: 0.1061 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f62900f9cf8>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Automatic one hot encoding\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jlOtLToLI_Cj"
   },
   "outputs": [],
   "source": [
    "#Save the model for later usage\n",
    "model.save_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tNVzVs4yI_Cl"
   },
   "outputs": [],
   "source": [
    "#Resuing the model by loading pre-trained model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Load the model's saved weights.\n",
    "model.load_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAHjn6Q9I_Cn",
    "outputId": "d6ba121a-7ab0-4364-a010-e2ba791806f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 9 3 9]\n",
      "[7 2 1 0 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "plt.imshow(test_images[4])\n",
    "# print(predictions)\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cmmmLN6I_Cp"
   },
   "source": [
    "Network Depth\n",
    "\n",
    "What happens if we add or remove Convolutional layers? How does that affect training and/or the model’s final performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zJ8n6AduI_Cp"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  Conv2D(num_filters, filter_size),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goeEFg1-I_Cr"
   },
   "source": [
    "Dropout\n",
    "\n",
    "What if we tried adding Dropout layers, which are commonly used to prevent overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "s5UkiKsbI_Cr"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN6fe6TI_Ct"
   },
   "source": [
    "Fully-connected Layers\n",
    "\n",
    "What if we add fully-connected layers between the Convolutional outputs and the final Softmax layer? This is something commonly done in CNNs used for Computer Vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JemJ3os5I_Ct"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2GwwNxKI_Cv"
   },
   "source": [
    "Convolution Parameters\n",
    "\n",
    "What if we play with the Conv2D parameters? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "j8ljjIfFI_Cv"
   },
   "outputs": [],
   "source": [
    "# These can be changed, too!\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "\n",
    "model = Sequential([\n",
    "  # See https://keras.io/layers/convolutional/#conv2d for more info.\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "  ),\n",
    "  MaxPooling2D(pool_size=pool_size, strides=2,),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZPIrvo1YI_Cx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MNIST_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
